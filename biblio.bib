
@article{Reed.Yu:TASSP:1990,
  title = {Adaptive Multiple-Band {{CFAR}} Detection of an Optical Pattern with Unknown Spectral Distribution},
  volume = {38},
  issn = {0096-3518},
  doi = {10.1109/29.60107},
  abstract = {A constant false alarm rate (CFAR) detection algorithm (see J.Y. Chen and I.S. Reed, IEEE Trans. Aerosp. Electron. Syst., vol.AES-23, no.1, Jan. 1987) is generalized to a test which is able to detect the presence of known optical signal pattern which has nonnegligible unknown relative intensities in several signal-plus-noise bands or channels. This test and its statistics are analytically evaluated, and the signal-to-noise ratio (SNR) performance improvement is analyzed. Both theoretical and computer simulation results show that the SNR improvement factor of this algorithm using multiple band scenes over the single scene of maximum SNR can be substantial. The SNR gain of this detection algorithm is compared to the previously published one. It illustrates that the generalized SNR of the test using the full data array is always greater than that of using partial data array. The database used to simulate this adaptive CFAR test is obtained from actual image scenes},
  number = {10},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  author = {Reed, I. S. and Yu, X.},
  month = oct,
  year = {1990},
  keywords = {Adaptive optics,optical radar,actual image scenes,adaptive multiple-band CFAR detection,constant false alarm rate,Detection algorithms,Electron optics,Layout,Optical detectors,optical signal pattern,Performance analysis,picture processing,Signal analysis,signal detection,Signal to noise ratio,signal-to-noise ratio,SNR gain,Statistical analysis,Testing},
  pages = {1760-1770}
}

@inproceedings{Witkin:ICASSP:1984,
  title = {Scale-Space Filtering: {{A}} New Approach to Multi-Scale Description},
  volume = {9},
  shorttitle = {Scale-Space Filtering},
  doi = {10.1109/ICASSP.1984.1172729},
  abstract = {The extrema in a signal and its first few derivatives provide a useful general purpose qualitative description for many kinds of signals. A fundamental problem in computing such descriptions is scale: a derivative must be taken over some neighborhood, but there is seldom a principled basis for choosing its size. Scale-space filtering is a method that describes signals qualitatively, managing the ambiguity of scale in an organized and natural way. The signal is first expanded by convolution with gaussian masks over a continuum of sizes. This "scale-space" image is then collapsed, using its qualitative structure, into a tree providing a concise but complete qualitative description covering all scales of observation. The description is further refined by applying a stability criterion, to identify events that persist of large changes in scale.},
  booktitle = {{{ICASSP}} '84. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Witkin, A.},
  month = mar,
  year = {1984},
  keywords = {Artificial intelligence,Filtering,Acoustic noise,Calculus,Convolution,Laboratories,Quality management,Signal processing,Smoothing methods,Stability criteria},
  pages = {150-153}
}

@book{Desolneux.Moisan.ea:Gestalt:2008,
  address = {New York},
  series = {Interdisciplinary Applied Mathematics},
  title = {From {{Gestalt Theory}} to {{Image Analysis}}: {{A Probabilistic Approach}}},
  isbn = {978-0-387-72635-9},
  shorttitle = {From {{Gestalt Theory}} to {{Image Analysis}}},
  abstract = {This book introduces the reader to a recent theory in Computer Vision yielding elementary techniques to analyse digital images. These techniques are inspired from and are a mathematical formalization of the Gestalt theory. Gestalt theory, which had never been formalized is a rigorous realm of vision psychology developped between 1923 and 1975. From the mathematical viewpoint the closest field to it is stochastic geometry, involving basic probability and statistics, in the context of image analysis. The book is intended for a multidisciplinary audience of researchers and engineers. It is self contained in three aspects: mathematics, vision and algorithms, and requires only a background of elementary calculus and probability. A large number of illustrations, exercises and examples are included. The authors maintain a public software, MegaWave, containing implementations of most of the image analysis techniques developed in the book.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Desolneux, Agn{\`e}s and Moisan, Lionel and Morel, Jean-Michel},
  year = {2008}
}

@article{Otsu:SMC:1979,
  title = {A {{Threshold Selection Method}} from {{Gray}}-{{Level Histograms}}},
  volume = {9},
  issn = {0018-9472},
  doi = {10.1109/TSMC.1979.4310076},
  number = {1},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  author = {Otsu, N.},
  month = jan,
  year = {1979},
  keywords = {Displays,Gaussian distribution,Histograms,Least squares approximation,Marine vehicles,Q measurement,Radar tracking,Sea measurements,Surveillance,Target tracking},
  pages = {62-66}
}

@article{Mumford.Shah:CPAM:1989,
  title = {Optimal Approximations by Piecewise Smooth Functions and Associated Variational Problems},
  volume = {42},
  issn = {1097-0312},
  doi = {10.1002/cpa.3160420503},
  language = {en},
  number = {5},
  journal = {Communications on Pure and Applied Mathematics},
  author = {Mumford, David and Shah, Jayant},
  month = jul,
  year = {1989},
  pages = {577-685}
}

@article{Hamming:BSTJ:1950,
  title = {Error Detecting and Error Correcting Codes},
  volume = {29},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1950.tb00463.x},
  abstract = {The author was led to the study given in this paper from a consideration of large scale computing machines in which a large number of operations must be performed without a single error in the end result. This problem of ``doing things right'' on a large scale is not essentially new; in a telephone central office, for example, a very large number of operations are performed while the errors leading to wrong numbers are kept well under control, though they have not been completely eliminated. This has been achieved, in part, through the use of self-checking circuits. The occasional failure that escapes routine checking is still detected by the customer and will, if it persists, result in customer complaint, while if it is transient it will produce only occasional wrong numbers. At the same time the rest of the central office functions satisfactorily. In a digital computer, on the other hand, a single failure usually means the complete failure, in the sense that if it is detected no more computing can be done until the failure is located and corrected, while if it escapes detection then it invalidates all subsequent operations of the machine. Put in other words, in a telephone central office there are a number of parallel paths which are more or less independent of each other; in a digital machine there is usually a single long path which passes through the same piece of equipment many, many times before the answer is obtained.},
  number = {2},
  journal = {The Bell System Technical Journal},
  author = {Hamming, R. W.},
  month = apr,
  year = {1950},
  pages = {147-160}
}

@article{Wertheimer:Psycologische:1923,
  title = {{{FormsUntersuchungen}} Zur {{Lehre}} von Der {{Gestalt II}}},
  volume = {4},
  journal = {Psycologische Forschung},
  author = {Wertheimer, Max},
  year = {1923},
  pages = {301--350}
}

@article{Frey.Dueck:SCIENCE:2017,
  title = {Clustering by Passing Messages between Data Points},
  volume = {315},
  issn = {1095-9203},
  doi = {10.1126/science.1136800},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such "exemplars" can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called "affinity propagation," which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
  language = {eng},
  number = {5814},
  journal = {Science (New York, N.Y.)},
  author = {Frey, Brendan J. and Dueck, Delbert},
  month = feb,
  year = {2017},
  keywords = {Cluster Analysis,Data Interpretation; Statistical,Face,Microarray Analysis},
  pages = {972-976}
}

@article{Sezgin.Sankur:EI:2010,
  title = {Survey over Image Thresholding Techniques and Quantitative Performance Evaluation.},
  volume = {13},
  number = {1},
  journal = {J. Electronic Imaging},
  author = {Sezgin, Mehmet and Sankur, B{\"u}lent},
  month = jul,
  year = {2010},
  keywords = {dblp},
  pages = {146-168}
}

@article{Lu.Chen.ea:IJAIT:2004,
  title = {Multivariate Spatial Outlier Detection},
  volume = {13},
  issn = {0218-2130},
  doi = {10.1142/S021821300400182X},
  abstract = {A spatial outlier is a spatially referenced object whose non-spatial attribute values are significantly different from the values of its neighborhood. Identification of spatial outliers can lead to the discovery of unexpected, interesting, and useful spatial patterns for further analysis. Previous work in spatial outlier detection focuses on detecting spatial outliers with a single attribute. In the paper, we propose two approaches to discover spatial outliers with multiple attributes. We formulate the multi-attribute spatial outlier detection problem in a general way, provide two effective detection algorithms, and analyze their computation complexity. In addition, using a real-world census data, we demonstrate that our approaches can effectively identify local abnormality in large spatial data sets.},
  number = {04},
  journal = {International Journal on Artificial Intelligence Tools},
  author = {Lu, Chang-Tien and Chen, Dechang and Kou, Yufeng},
  month = dec,
  year = {2004},
  pages = {801-811}
}

@article{Attneave:PR:1954,
  title = {Some Informational Aspects of Visual Perception.},
  volume = {61},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/h0054663},
  abstract = {This is an attempt to apply the concepts and techniques of information theory to the problems of visual perception. The informational concept of redundancy comes in for a good deal of attention with regard to the understanding of phenomena of visual perception, and a demonstration of its nature in this area is presented. The analysis employed by the author also permits him to present informational and statistical descriptions of a good many classical concepts from the area of vision, including the historically most important Gestalt perceptual principles. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {3},
  journal = {Psychological Review},
  author = {Attneave, Fred},
  year = {1954},
  keywords = {Statistics,*Visual Perception,Information Theory},
  pages = {183-193}
}

@article{Marr.Hildreth:PRS:1980,
  title = {Theory of Edge Detection},
  volume = {207},
  copyright = {Scanned images copyright \textcopyright{} 2017, Royal Society},
  issn = {0080-4649, 2053-9193},
  doi = {10.1098/rspb.1980.0020},
  abstract = {A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of $\nabla$2G(x, y)* I(x, y) for image I, where G(x, y) is a two-dimen$\-$sional Gaussian distribution and $\nabla$2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination bound$\-$aries, and these all have the property that they are spatially localized. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the opera$\-$tion of forming oriented zero-crossing segments from the output of centre-surround $\nabla$2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr \& Ullman 1979).},
  language = {en},
  number = {1167},
  journal = {Proc. R. Soc. Lond. B},
  author = {Marr, D. and Hildreth, E.},
  month = feb,
  year = {1980},
  pages = {187-217}
}

@book{Petitot:Neurogeometrie:2008,
  title = {{Neurog{\'e}om{\'e}trie de la vision: mod{\`e}les math{\'e}matiques et physiques des architectures fonctionnelles}},
  isbn = {978-2-7302-1507-7},
  shorttitle = {{Neurog{\'e}om{\'e}trie de la vision}},
  abstract = {Approfondissant le cours de Sciences cognitives de l'Ecole Polytechnique, cet ouvrage expose, en relation {\'e}troite avec un vaste ensemble de donn{\'e}es exp{\'e}rimentales, plusieurs mod{\`e}les physico-math{\'e}matiques du cortex visuel primaire (aire V1) et propose un mod{\`e}le g{\'e}om{\'e}trique original de son architecture fonctionnelle, c'est-{\`a}-dire de l'organisation de ses connexions neuronales. Son propos est d'expliciter les algorithmes g{\'e}om{\'e}triques que cette architecture fonctionnelle impl{\'e}mente, autrement dit la " neurog{\'e}om{\'e}trie " immanente {\`a} la perception visuelle. Il concerne donc en d{\'e}finitive l'origine neuronale des repr{\'e}sentations spatiales. Techniquement, il montre essentiellement trois choses. D'abord que le filtrage du signal optique par les neurones visuels s'apparente {\`a} une analyse en ondelettes. Ensuite que la structure de contact de l'espace des 1-jets des courbes du plan (ici le plan r{\'e}tinien) se trouve impl{\'e}ment{\'e}e par l'architecture fonctionnelle corticale. Enfin que les algorithmes visuels d'int{\'e}gration des contours {\`a} partir de donn{\'e}es sensorielles {\'e}ventuellement tr{\`e}s lacunaires sont mod{\'e}lisables en termes de la g{\'e}om{\'e}trie sous-riemannienne associ{\'e}e {\`a} cette structure de contact. L'ouvrage offre ainsi au lecteur la premi{\`e}re interpr{\'e}tation syst{\'e}matique d'un nombre important de donn{\'e}es neurophysiologiques dans un cadre math{\'e}matique bien d{\'e}fini, celui des {\'e}tats coh{\'e}rents sur le groupe E(2) des d{\'e}placements du plan et celui de la g{\'e}om{\'e}trie sous-riemannienne. Dans la mesure o{\`u} l'origine des repr{\'e}sentations spatiales constitue un probl{\`e}me majeur non seulement scientifique mais aussi philosophique, l'ouvrage poss{\`e}de {\'e}galement une forte dimension {\'e}pist{\'e}mologique. Dit bri{\`e}vement, il vise {\`a} montrer que la neurog{\'e}om{\'e}trie de l'espace sensible permet de clarifier ce que l'on appelle en philosophie transcendantale le caract{\`e}re synth{\'e}tique a priori de l'espace.},
  language = {fr},
  publisher = {{Editions Ecole Polytechnique}},
  author = {Petitot, Jean},
  year = {2008}
}

@inproceedings{Lee.Wang.ea:IRC:2017,
  title = {Real-{{Time}}, {{Cloud}}-{{Based Object Detection}} for {{Unmanned Aerial Vehicles}}},
  doi = {10.1109/IRC.2017.77},
  abstract = {Real-time object detection is crucial for many applications of Unmanned Aerial Vehicles (UAVs) such as reconnaissance and surveillance, search-and-rescue, and infrastructure inspection. In the last few years, Convolutional Neural Networks (CNNs) have emerged as a powerful class of models for recognizing image content, and are widely considered in the computer vision community to be the de facto standard approach for most problems. However, object detection based on CNNs is extremely computationally demanding, typically requiring high-end Graphics Processing Units (GPUs) that require too much power and weight, especially for a lightweight and low-cost drone. In this paper, we propose moving the computation to an off-board computing cloud, while keeping low-level object detection and short-term navigation onboard. We apply Faster Regions with CNNs (R-CNNs), a state-of-the-art algorithm, to detect not one or two but hundreds of object types in near real-time.},
  booktitle = {2017 {{First IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Lee, J. and Wang, J. and Crandall, D. and {\v S}abanovi{\'c}, S. and Fox, G.},
  month = apr,
  year = {2017},
  keywords = {Cameras,object detection,mobile robots,robot vision,aerospace control,autonomous aerial vehicles,cloud computing,Cloud computing,cloud-based object detection,CNN,computer vision community,convolutional neural networks,Convolutional Neural Networks,de facto standard approach,Drones,Estimation,GPU,graphics processing units,image content recognition,neurocontrollers,Object detection,Object Detection,real-time object detection,real-time systems,Real-time systems,Robot Vision,Robots,telerobotics,UAV,Unmanned Aerial Systems,unmanned aerial vehicles},
  pages = {36-43}
}

@article{Araar.Aouf.ea:IROS:2017,
  title = {Vision {{Based Autonomous Landing}} of {{Multirotor UAV}} on {{Moving Platform}}},
  volume = {85},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-016-0399-z},
  abstract = {This paper investigates solutions for the fundamental yet challenging problem of autonomous landing of multirotor Unammaned Aerial Vehicles UAVs. In addition to landing on static targets, tracking and landing on a moving platform is addressed, as a solution to facilitate the deployment of the UAV. The paper presents the design of a new landing pad and its relative pose estimation. The fusion of inertial measurement with the estimated pose is considered to ensure a high sampling rate, and to increase the manoeuvrability of the vehicle. Two filters are designed to conduct the fusion, an Extended Kalman Filter (EKF) and an Extended H$\infty$ (EH$\infty$). The extensive simulation and practical tests permitted identification of the challenges of the landing task. Adequate solutions to these challenges are proposed to lessen their impact on landing precision.},
  language = {en},
  number = {2},
  journal = {Journal of Intelligent \& Robotic Systems},
  author = {Araar, Oualid and Aouf, Nabil and Vitanov, Ivan},
  month = feb,
  year = {2017},
  pages = {369-384}
}

@inproceedings{Lacroix.Caballero:IROS:2006,
  title = {Autonomous Detection of Safe Landing Areas for an {{UAV}} from Monocular Images},
  abstract = {Abstract\textemdash{}This paper presents an approach to detect safe landing areas for a flying robot, on the basis of a sequence of monocular images. The approach does not require precise position and attitude sensors: it exploits the relations between 2D image homographies and 3D planes. The combination of a robust homography estimation and of an adaptive thresholding of correlation scores between registered images yields the update of a stochastic grid, that exhibits the horizontal planar areas perceived. This grid allows the integration of data gathered at various altitudes. Results are presented throughout the article. I.},
  booktitle = {In {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Lacroix, Simon and Caballero, Fernando},
  year = {2006}
}

@inproceedings{Lange.Sunderhauf.ea:SIMPAR:2008,
  title = {Autonomous {{Landing}} for a {{Multirotor UAV Using Vision}}},
  abstract = {Abstract. We describe our work on multirotor UAVs and focus on our method for autonomous landing. The paper describes the design of our landing pad and its advantages. We explain how the landing pad detection algorithm works and how the 3D-position of the UAV relative to the landing pad is calculated. Practical experiments prove the quality of these estimations. 1},
  booktitle = {In {{SIMPAR}} 2008 {{Intl}}. {{Conf}}. on {{Simulation}}, {{Modeling}} and {{Programming}} for {{Autonomous Robots}}},
  author = {Lange, Sven and S{\"u}nderhauf, Niko and Protzel, Peter},
  year = {2008},
  pages = {482--491}
}

@article{Carrio.Sampedro.ea:JS:2017,
  title = {A {{Review}} of {{Deep Learning Methods}} and {{Applications}} for {{Unmanned Aerial Vehicles}}},
  volume = {2017},
  journal = {Journal of Sensors},
  author = {Carrio, Adrian and Sampedro, Carlos and Rodriguez-Ramos, Alejandro and Cervera, Pascual Campoy},
  year = {2017},
  pages = {3296874:1-3296874:13}
}

@article{Furukawa:TechRep:2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.08558},
  primaryClass = {cs},
  title = {Deep {{Learning}} for {{End}}-to-{{End Automatic Target Recognition}} from {{Synthetic Aperture Radar Imagery}}},
  abstract = {The standard architecture of synthetic aperture radar (SAR) automatic target recognition (ATR) consists of three stages: detection, discrimination, and classification. In recent years, convolutional neural networks (CNNs) for SAR ATR have been proposed, but most of them classify target classes from a target chip extracted from SAR imagery, as a classification for the third stage of SAR ATR. In this report, we propose a novel CNN for end-to-end ATR from SAR imagery. The CNN named verification support network (VersNet) performs all three stages of SAR ATR end-to-end. VersNet inputs a SAR image of arbitrary sizes with multiple classes and multiple targets, and outputs a SAR ATR image representing the position, class, and pose of each detected target. This report describes the evaluation results of VersNet which trained to output scores of all 12 classes: 10 target classes, a target front class, and a background class, for each pixel using the moving and stationary target acquisition and recognition (MSTAR) public dataset.},
  journal = {arXiv:1801.08558 [cs]},
  author = {Furukawa, Hidetoshi},
  month = jan,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{Yao.Yu.ea:CCC:2017,
  title = {Deep-Learning-Based Moving Target Detection for Unmanned Air Vehicles},
  doi = {10.23919/ChiCC.2017.8029186},
  abstract = {In this paper, a deep learning network is investigated to detect moving targets for a UAV equipped with monocular camera. An algorithm based on fully convolutional network is proposed to obtain the position and moving direction of targets. A Kalman filter is incorporated into the proposed algorithm to increase the accuracy of target position information acquisition. The experimental results show the effectiveness of the proposed algorithm with a relatively low hardware resource consumption.},
  booktitle = {2017 36th {{Chinese Control Conference}} ({{CCC}})},
  author = {Yao, H. and Yu, Q. and Xing, X. and He, F. and Ma, J.},
  month = jul,
  year = {2017},
  keywords = {Cameras,object detection,Unmanned aerial vehicles,autonomous aerial vehicles,Robots,UAV,aerospace computing,cameras,Classification algorithms,deep learning,deep learning-based moving target detection,fully convolutional network,Image color analysis,image filtering,Kalman filter,Kalman filters,learning (artificial intelligence),low hardware resource consumption,Machine learning,monocular camera,target position information acquisition,unmanned air vehicle,unmanned air vehicles},
  pages = {11459-11463}
}

@article{Lu.Xue.ea:GSIS:2018,
  title = {A Survey on Vision-Based {{UAV}} Navigation},
  volume = {21},
  issn = {1009-5020},
  doi = {10.1080/10095020.2017.1420509},
  abstract = {Research on unmanned aerial vehicles (UAV) has been increasingly popular in the past decades, and UAVs have been widely used in industrial inspection, remote sensing for mapping \& surveying, rescuing, and so on. Nevertheless, the limited autonomous navigation capability severely hampers the application of UAVs in complex environments, such as GPS-denied areas. Previously, researchers mainly focused on the use of laser or radar sensors for UAV navigation. With the rapid development of computer vision, vision-based methods, which utilize cheaper and more flexible visual sensors, have shown great advantages in the field of UAV navigation. The purpose of this article is to present a comprehensive literature review of the vision-based methods for UAV navigation. Specifically on visual localization and mapping, obstacle avoidance and path planning, which compose the essential parts of visual navigation. Furthermore, throughout this article, we will have an insight into the prospect of the UAV navigation and the challenges to be faced.},
  number = {1},
  journal = {Geo-spatial Information Science},
  author = {Lu, Yuncheng and Xue, Zhucun and Xia, Gui-Song and Zhang, Liangpei},
  month = jan,
  year = {2018},
  keywords = {obstacle avoidance,path planning,Unmanned aerial vehicles (UAV),visual SLAM},
  pages = {21-32}
}

@article{Stefas.Bayram.ea::2016,
  series = {5th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2016},
  title = {Vision-{{Based UAV Navigation}} in {{Orchards}}},
  volume = {49},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2016.10.003},
  abstract = {Unmanned Aerial Vehicles (UAV) are becoming increasingly common in agricultural applications. Currently, they are primarily used to fly over fields in open space. Navigation inside orchard-like environments remains challenging. We study the problem of orchard navigation with cameras on an aerial vehicle. We study both the controller and the vision component. For the vision component, we provide two methods for detecting orchard rows with frontal facing cameras. In the monocular case, we present a pipeline to extract the geometry of tree rows when there is a well defined path structure. In the binocular case, we present a depth-based navigation algorithm to extract the rows. For the controller component, we design a controller that uses both frontal and downward facing cameras and provides reliable performance even on the presence of strong wind disturbances.},
  number = {16},
  journal = {IFAC-PapersOnLine},
  author = {Stefas, Nikolaos and Bayram, Haluk and Isler, Volkan},
  month = jan,
  year = {2016},
  keywords = {Agricultural Robotics,Unmanned Aerial Vehicles,Vision-based navigation},
  pages = {10-15}
}

@article{Cesetti.Frontoni.ea::2010,
  title = {A {{Vision}}-{{Based Guidance System}} for {{UAV Navigation}} and {{Safe Landing}} Using {{Natural Landmarks}}},
  volume = {57},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-009-9373-3},
  abstract = {In this paper a vision-based approach for guidance and safe landing of an Unmanned Aerial Vehicle (UAV) is proposed. The UAV is required to navigate from an initial to a final position in a partially known environment. The guidance system allows a remote user to define target areas from a high resolution aerial or satellite image to determine either the waypoints of the navigation trajectory or the landing area. A feature-based image-matching algorithm finds the natural landmarks and gives feedbacks to an onboard, hierarchical, behaviour-based control system for autonomous navigation and landing. Two algorithms for safe landing area detection are also proposed, based on a feature optical flow analysis. The main novelty is in the vision-based architecture, extensively tested on a helicopter, which, in particular, does not require any artificial landmark (e.g., helipad). Results show the appropriateness of the vision-based approach, which is robust to occlusions and light variations.},
  language = {en},
  number = {1-4},
  journal = {Journal of Intelligent and Robotic Systems},
  author = {Cesetti, A. and Frontoni, E. and Mancini, A. and Zingaretti, P. and Longhi, S.},
  month = jan,
  year = {2010},
  pages = {233}
}

@inproceedings{Sinopoli.Micheli.ea::2001,
  title = {Vision Based Navigation for an Unmanned Aerial Vehicle},
  volume = {2},
  doi = {10.1109/ROBOT.2001.932864},
  abstract = {We are developing a system for autonomous navigation of unmanned aerial vehicles (UAVs) based on computer vision. A UAV is equipped with on-board cameras and each UAV is provided with noisy estimates of its own state, coming from GPS/INS. The mission of the UAV is low altitude navigation from an initial position to a final position in a partially known 3-D environment while avoiding obstacles and minimizing path length. We use a hierarchical approach to path planning. We distinguish between a global offline computation, based on a coarse known model of the environment and a local online computation, based on the information coming from the vision system. A UAV builds and updates a virtual 3-D model of the surrounding environment by processing image sequences and fusing them with sensor data. Based on such a model the UAV will plan a path from its current position to the terminal point. It will then follow such path, getting more data from the on-board cameras, and refining map and local path in real time.},
  booktitle = {Proceedings 2001 {{ICRA}}. {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{01CH37164}})},
  author = {Sinopoli, B. and Micheli, M. and Donato, G. and Koo, T. J.},
  year = {2001},
  keywords = {Machine vision,Computer vision,Bayes methods,Cameras,image sequences,aerospace robotics,Global Positioning System,mobile robots,remotely operated vehicles,robot vision,unmanned aerial vehicle,Unmanned aerial vehicles,path planning,autonomous navigation,coarse known environment model,computer vision,dynamic programming,global offline computation,GPS/INS,hierarchical approach,Image sequences,local online computation,low altitude navigation,Navigation,on-board cameras,partially known 3D environment,Path planning,probability,sensor fusion,State estimation,virtual 3D model,vision based navigation,wavelet transforms,Working environment noise},
  pages = {1757-1764 vol.2}
}

@book{Chatterjee.Rakshit.ea::2013,
  address = {Berlin Heidelberg},
  series = {Studies in Computational Intelligence},
  title = {Vision {{Based Autonomous Robot Navigation}}: {{Algorithms}} and {{Implementations}}},
  isbn = {978-3-642-33964-6},
  shorttitle = {Vision {{Based Autonomous Robot Navigation}}},
  abstract = {This monograph is devoted to the theory and development of autonomous navigation of mobile robots using computer vision based sensing mechanism. The conventional robot navigation systems, utilizing traditional sensors like ultrasonic, IR, GPS, laser sensors etc., suffer several drawbacks related to either the physical limitations of the sensor or incur high cost. Vision sensing has emerged as a popular alternative where cameras can be used to reduce the overall cost, maintaining high degree of intelligence, flexibility and robustness.This book includes a detailed description of several new approaches for real life vision based autonomous navigation algorithms and SLAM. It presents the concept of how subgoal based goal-driven navigation can be carried out using vision sensing. The development concept of vision based robots for path/line tracking using fuzzy logic is presented, as well as how a low-cost robot can be indigenously developed in the laboratory with microcontroller based sensor systems. The book describes successful implementation of integration of low-cost, external peripherals, with off-the-shelf procured robots. An important highlight of the book is that it presents a detailed, step-by-step sample demonstration of how vision-based navigation modules can be actually implemented in real life, under 32-bit Windows environment. The book also discusses the concept of implementing vision based SLAM employing a two camera based system.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Chatterjee, Amitava and Rakshit, Anjan and Singh, N. Nirmal},
  year = {2013}
}

@inproceedings{Klein.Murray:ISMAR:2007,
  title = {Parallel {{Tracking}} and {{Mapping}} for {{Small AR Workspaces}}},
  doi = {10.1109/ISMAR.2007.4538852},
  abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
  booktitle = {2007 6th {{IEEE}} and {{ACM International Symposium}} on {{Mixed}} and {{Augmented Reality}}},
  author = {Klein, G. and Murray, D.},
  month = nov,
  year = {2007},
  keywords = {Algorithm design and analysis,augmented reality,batch optimisation techniques,Cameras,Concurrent computing,hand-held camera,Handheld computers,Layout,parallel mapping,parallel tracking,robot vision,Robot vision systems,robotic exploration,Robustness,Simultaneous localization and mapping,SLAM (robots),SLAM algorithms,Tracking,Yarn},
  pages = {225-234}
}

@article{Harmat.Trentini.ea:IROS:2015,
  title = {Multi-{{Camera Tracking}} and {{Mapping}} for {{Unmanned Aerial Vehicles}} in {{Unstructured Environments}}},
  volume = {78},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-014-0085-y},
  abstract = {Pose estimation for small unmanned aerial vehicles has made large improvements in recent years, leading to vehicles that use a suite of sensors to navigate and explore various environments. In particular, cameras have become popular due to their low weight and power consumption, as well as the large amount of data they capture. However, processing this data to extract useful information has proved challenging, as the pose estimation problem is inherently nonlinear and, depending on the cameras' field of view, potentially ill-posed. Results from the field of multi-camera egomotion estimation show that these issues can be reduced or eliminated by using multiple cameras positioned appropriately. In this work, we make use of these insights to develop a multi-camera visual pose estimator using ultra wide angle fisheye cameras, leading to a system that has many advantages over traditional visual pose estimators. The system is tested in a variety of configurations and flight scenarios on an unprepared urban rooftop, including landings and takeoffs. To our knowledge, this is the first time a visual pose estimator has been shown to be able to continuously track the pose of a small aerial vehicle throughout the landing and subsequent takeoff maneuvers.},
  language = {en},
  number = {2},
  journal = {Journal of Intelligent \& Robotic Systems},
  author = {Harmat, Adam and Trentini, Michael and Sharf, Inna},
  month = may,
  year = {2015},
  pages = {291-317}
}

@book{Nonami.Kendoul.ea:AutonomFlightRobots:2010,
  title = {Autonomous {{Flying Robots}}: {{Unmanned Aerial Vehicles}} and {{Micro Aerial Vehicles}}},
  isbn = {978-4-431-53855-4},
  shorttitle = {Autonomous {{Flying Robots}}},
  abstract = {The advance in robotics has boosted the application of autonomous vehicles to perform tedious and risky tasks or to be cost-effective substitutes for their - man counterparts. Based on their working environment, a rough classi cation of the autonomous vehicles would include unmanned aerial vehicles (UAVs), - manned ground vehicles (UGVs), autonomous underwater vehicles (AUVs), and autonomous surface vehicles (ASVs). UAVs, UGVs, AUVs, and ASVs are called UVs (unmanned vehicles) nowadays. In recent decades, the development of - manned autonomous vehicles have been of great interest, and different kinds of autonomous vehicles have been studied and developed all over the world. In part- ular, UAVs have many applications in emergency situations; humans often cannot come close to a dangerous natural disaster such as an earthquake, a ood, an active volcano, or a nuclear disaster. Since the development of the rst UAVs, research efforts have been focused on military applications. Recently, however, demand has arisen for UAVs such as aero-robotsand ying robotsthat can be used in emergency situations and in industrial applications. Among the wide variety of UAVs that have been developed, small-scale HUAVs (helicopter-based UAVs) have the ability to take off and land vertically as well as the ability to cruise in ight, but their most importantcapability is hovering. Hoveringat a point enables us to make more eff- tive observations of a target. Furthermore, small-scale HUAVs offer the advantages of low cost and easy operation.},
  language = {en},
  publisher = {{Springer Japan}},
  author = {Nonami, Kenzo and Kendoul, Farid and Suzuki, Satoshi and Wang, Wei and Nakazawa, Daisuke},
  year = {2010}
}

@inproceedings{Forster.Pizzoli.ea:ICRA:2014,
  title = {{{SVO}}: {{Fast}} Semi-Direct Monocular Visual Odometry},
  shorttitle = {{{SVO}}},
  doi = {10.1109/ICRA.2014.6906584},
  abstract = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state-estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Forster, C. and Pizzoli, M. and Scaramuzza, D.},
  month = may,
  year = {2014},
  keywords = {3D points,autonomous aerial vehicles,Cameras,consumer laptop,control engineering computing,distance measurement,embedded systems,fast semidirect monocular visual odometry,Feature extraction,GPS-denied environments,high frame-rate motion estimation,micro-aerial-vehicle state-estimation,motion estimation,Motion estimation,onboard embedded computer,open-source software,Optimization,outlier measurements,pixel intensities,probabilistic mapping method,probability,robot vision,Robustness,stereo image processing,subpixel precision,SVO,Three-dimensional displays,Tracking},
  pages = {15-22}
}

@inproceedings{Lowe:ICCV:1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  volume = {2},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D. G.},
  year = {1999},
  keywords = {3D projection,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,computational geometry,Computer science,Electrical capacitance tomography,feature extraction,Filters,image matching,Image recognition,inferior temporal cortex,Layout,least squares approximations,Lighting,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,Neurons,object recognition,Object recognition,primate vision,Programmable logic arrays,Reactive power,robust object recognition,staged filtering approach,unknown model parameters},
  pages = {1150-1157 vol.2}
}

@inproceedings{Rosten.Drummond:ECCV:2006,
  series = {Lecture Notes in Computer Science},
  title = {Machine {{Learning}} for {{High}}-{{Speed Corner Detection}}},
  isbn = {978-3-540-33832-1 978-3-540-33833-8},
  doi = {10.1007/11744023_34},
  abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7\% of the available processing time. By comparison neither the Harris detector (120\%) nor the detection stage of SIFT (300\%) can operate at full frame rate.Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2006},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Rosten, Edward and Drummond, Tom},
  month = may,
  year = {2006},
  pages = {430-443}
}

@article{Bay.Ess.ea:CVIU:2008,
  series = {Similarity Matching in Computer Vision and Multimedia},
  title = {Speeded-{{Up Robust Features}} ({{SURF}})},
  volume = {110},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2007.09.014},
  abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.},
  number = {3},
  journal = {Computer Vision and Image Understanding},
  author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
  month = jun,
  year = {2008},
  keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
  pages = {346-359}
}

@inproceedings{Harris.Stephens:AVC:1988,
  title = {A Combined Corner and Edge Detector},
  abstract = {Consistency of image edge filtering is of prime importance for 3D interpretation of image sequences using feature tracking algorithms. To cater for image regions containing texture and isolated features, a combined corner and edge detector based on the local auto-correlation function is utilised, and it is shown to perform with good consistency on natural imagery.},
  booktitle = {In {{Proc}}. of {{Fourth Alvey Vision Conference}}},
  author = {Harris, Chris and Stephens, Mike},
  year = {1988},
  pages = {147--151}
}

@article{Scaramuzza.Fraundorfer:RAM:2011,
  title = {Visual {{Odometry}} [{{Tutorial}}]},
  volume = {18},
  issn = {1070-9932},
  doi = {10.1109/MRA.2011.943233},
  abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.},
  number = {4},
  journal = {IEEE Robotics Automation Magazine},
  author = {Scaramuzza, D. and Fraundorfer, F.},
  month = dec,
  year = {2011},
  keywords = {augmented reality,automotive,cameras,egomotion estimation,illumination,image texture,motion estimation,multiple cameras,onboard cameras,pose estimation,road vehicles,robotics,robots,scene overlap,single camera,vehicle,visual odometry,wearable computing,wheel odometry,wheels},
  pages = {80-92}
}

@inproceedings{Molton.Davison.ea:BMVC:2004,
  title = {Locally {{Planar Patch Features}} for {{Real}}-{{Time Structure}} from {{Motion}}},
  booktitle = {Proc. {{British Machine Vision Conference}}},
  publisher = {{BMVC}},
  author = {Molton, N. D. and Davison, A. J. and Reid, I. D.},
  month = sep,
  year = {2004}
}

@article{Jin.Favaro.ea:VC:2003,
  title = {A Semi-Direct Approach to Structure from Motion},
  volume = {19},
  issn = {0178-2789, 1432-2315},
  doi = {10.1007/s00371-003-0202-6},
  abstract = {The problem of structure from motion is often decomposed into two steps: feature correspondence and three-dimensional reconstruction. This separation often causes gross errors when establishing correspondence fails. Therefore, we advocate the necessity to integrate visual information not only in time (i.e. across different views), but also in space, by matching regions \textendash{} rather than points \textendash{} using explicit photometric deformation models. We present an algorithm that integrates image-feature tracking and three-dimensional motion estimation into a closed loop, while detecting and rejecting outlier regions that do not fit the model. Due to occlusions and the causal nature of our algorithm, a drift in the estimates accumulates over time. We describe a method to perform global registration of local estimates of motion and structure by matching the appearance of feature regions stored over long time periods. We use image intensities to construct a score function that takes into account changes in brightness and contrast. Our algorithm is recursive and suitable for real-time implementation.},
  language = {en},
  number = {6},
  journal = {The Visual Computer},
  author = {Jin, Hailin and Favaro, Paolo and Soatto, Stefano},
  month = oct,
  year = {2003},
  pages = {377-394}
}

@article{Lovegrove.Davison.ea:IVS:2011,
  title = {Accurate Visual Odometry from a Rear Parking Camera},
  journal = {2011 IEEE Intelligent Vehicles Symposium (IV)},
  author = {Lovegrove, Steven and Davison, Andrew J. and Guzman, Javier Ibanez},
  year = {2011},
  pages = {788-793}
}

@inproceedings{Newcombe.Lovegrove.ea:ICCV:2011,
  title = {{{DTAM}}: {{Dense}} Tracking and Mapping in Real-Time},
  shorttitle = {{{DTAM}}},
  doi = {10.1109/ICCV.2011.6126513},
  abstract = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Newcombe, R. A. and Lovegrove, S. J. and Davison, A. J.},
  month = nov,
  year = {2011},
  keywords = {augmented reality,cameras,Cameras,concave programming,dense model,dense tracking and mapping,energy functional,GPU hardware,graphics processing units,hand-held RGB camera,image alignment,image motion analysis,image reconstruction,Image reconstruction,image texture,nonconvex optimisation,object tracking,Optimization,photometric data term,physics-enhanced augmented reality,Real time systems,real-time camera reconstruction,real-time camera tracking,real-time scene interaction,Robustness,textured depth map,Tracking,Vectors,video stream},
  pages = {2320-2327}
}

@inproceedings{Moulon.Monasse.ea:ACCV:2012,
  series = {Lecture Notes in Computer Science},
  title = {Adaptive {{Structure}} from {{Motion}} with a {{Contrario Model Estimation}}},
  isbn = {978-3-642-37446-3 978-3-642-37447-0},
  doi = {10.1007/978-3-642-37447-0_20},
  abstract = {Structure from Motion (SfM) algorithms take as input multi-view stereo images (along with internal calibration information) and yield a 3D point cloud and camera orientations/poses in a common 3D coordinate system. In the case of an incremental SfM pipeline, the process requires repeated model estimations based on detected feature points: homography, fundamental and essential matrices, as well as camera poses. These estimations have a crucial impact on the quality of 3D reconstruction. We propose to improve these estimations using the a contrario methodology. While SfM pipelines usually have globally-fixed thresholds for model estimation, the a contrario principle adapts thresholds to the input data and for each model estimation. Our experiments show that adaptive thresholds reach a significantly better precision. Additionally, the user is free from having to guess thresholds or to optimistically rely on default values. There are also cases where a globally-fixed threshold policy, whatever the threshold value is, cannot provide the best accuracy, contrary to an adaptive threshold policy.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ACCV}} 2012},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Moulon, Pierre and Monasse, Pascal and Marlet, Renaud},
  month = nov,
  year = {2012},
  pages = {257-270}
}

@article{Kanellakis.Nikolakopoulos:IROS:2017,
  title = {Survey on {{Computer Vision}} for {{UAVs}}: {{Current Developments}} and {{Trends}}},
  volume = {87},
  issn = {0921-0296, 1573-0409},
  shorttitle = {Survey on {{Computer Vision}} for {{UAVs}}},
  doi = {10.1007/s10846-017-0483-z},
  abstract = {During last decade the scientific research on Unmanned Aerial Vehicless (UAVs) increased spectacularly and led to the design of multiple types of aerial platforms. The major challenge today is the development of autonomously operating aerial agents capable of completing missions independently of human interaction. To this extent, visual sensing techniques have been integrated in the control pipeline of the UAVs in order to enhance their navigation and guidance skills. The aim of this article is to present a comprehensive literature review on vision based applications for UAVs focusing mainly on current developments and trends. These applications are sorted in different categories according to the research topics among various research groups. More specifically vision based position-attitude control, pose estimation and mapping, obstacle detection as well as target tracking are the identified components towards autonomous agents. Aerial platforms could reach greater level of autonomy by integrating all these technologies onboard. Additionally, throughout this article the concept of fusion multiple sensors is highlighted, while an overview on the challenges addressed and future trends in autonomous agent development will be also provided.},
  language = {en},
  number = {1},
  journal = {Journal of Intelligent \& Robotic Systems},
  author = {Kanellakis, Christoforos and Nikolakopoulos, George},
  month = jul,
  year = {2017},
  pages = {141-168}
}

@article{Srinivasan.Gregory:PTBS:1992,
  title = {How {{Bees Exploit Optic Flow}}: {{Behavioural Experiments}} and {{Neural Models}} [and  {{Discussion}}]},
  volume = {337},
  issn = {0962-8436},
  shorttitle = {How {{Bees Exploit Optic Flow}}},
  abstract = {Over the past thirty or so years, motion processing in insects has been studied primarily through the `optomotor response', a turning response evoked by the movement of a large-field visual pattern. More recently, however, evidence is accumulating to suggest that, in addition to the optomotor pathway, there are other pathways which use motion information in subtler ways. When an insect moves in a stationary environment, the resulting optic flow field is rich in information that can be exploited to estimate the distance to a surface, distinguish between objects at different distances, land on a contrasting edge, or distinguish an object from a similarly textured background. This article reviews recent behavioural studies in our laboratory, investigating how honeybees accomplish such tasks.},
  number = {1281},
  journal = {Philosophical Transactions: Biological Sciences},
  author = {Srinivasan, Mandyam V. and Gregory, R. L.},
  year = {1992},
  pages = {253-259}
}

@inproceedings{Franceschini.Ruffier.ea:InTech:2012,
  title = {Optic {{Flow Based Visual Guidance}}: {{From Flying Insects}} to {{Miniature Aerial Vehicles}}},
  author = {Franceschini, Nicolas H. and Ruffier, Franck and Serres, Julien and Viollet, St{\'e}phane},
  year = {2012}
}

@inproceedings{Al-Kaff.Meng.ea:IVS:2016,
  title = {Monocular Vision-Based Obstacle Detection/Avoidance for Unmanned Aerial Vehicles},
  doi = {10.1109/IVS.2016.7535370},
  abstract = {Robust real-time obstacle detection/avoidance is a challenging problem especially for micro and small aerial vehicles due to the limited number of the on-board sensors due to the battery constraint and low payload. Usually lightweight sensors such as CMOS camera are the best choice comparing with laser or radar sensors. For real-time applications, most studies focus on using stereo cameras to reconstruct a 3D model of the obstacles or to estimate their depth. Instead, in this paper, a method that mimics the human behavior of detecting the state of the approaching obstacles using single camera is proposed. During the flight, this method is able to detect the changes of the size area of the obstacles. First, the method detects the feature points of the obstacles, and then extracts the obstacles that has probability of getting close. In addition, by comparing the changes in the area ratios of the obstacle in the image sequence, the method can decide if it is obstacle or not. Finally, by estimating the obstacle 2D position in the image and combining with the tracked waypoints, the UAV can take the action of avoidance.},
  booktitle = {{{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Al-Kaff, A. and Meng, Qinggang and Mart{\'\i}n, D. and de la Escalera, A. and Armingol, J. M.},
  month = jun,
  year = {2016},
  keywords = {feature extraction,Laser radar,Feature extraction,Cameras,image sequences,object detection,mobile robots,robot vision,autonomous aerial vehicles,unmanned aerial vehicles,cameras,Navigation,Three-dimensional displays,pose estimation,CMOS camera,CMOS image sensors,collision avoidance,image sequence,lightweight sensors,microaerial vehicles,monocular vision-based obstacle avoidance,monocular vision-based obstacle detection,obstacle 2D position estimation,obstacles feature point extraction,on-board sensors,Sensors,small aerial vehicles,Solid modeling},
  pages = {92-97}
}

@article{Gageik.Benz.ea:ACCESS:2015,
  title = {Obstacle {{Detection}} and {{Collision Avoidance}} for a {{UAV With Complementary Low}}-{{Cost Sensors}}},
  volume = {3},
  doi = {10.1109/ACCESS.2015.2432455},
  abstract = {This paper demonstrates an innovative and simple solution for obstacle detection and collision avoidance of unmanned aerial vehicles (UAVs) optimized for and evaluated with quadrotors. The sensors exploited in this paper are low-cost ultrasonic and infrared range finders, which are much cheaper though noisier than more expensive sensors such as laser scanners. This needs to be taken into consideration for the design, implementation, and parametrization of the signal processing and control algorithm for such a system, which is the topic of this paper. For improved data fusion, inertial and optical flow sensors are used as a distance derivative for reference. As a result, a UAV is capable of distance controlled collision avoidance, which is more complex and powerful than comparable simple solutions. At the same time, the solution remains simple with a low computational burden. Thus, memory and time-consuming simultaneous localization and mapping is not required for collision avoidance.},
  journal = {IEEE Access},
  author = {Gageik, N. and Benz, P. and Montenegro, S.},
  year = {2015},
  keywords = {autonomous,autonomous aerial vehicles,collision avoidance,Collision avoidance,complementary low-cost sensors,distance measurement,helicopters,infrared,Infrared detection,infrared detectors,infrared range finders,low-cost ultrasonic range finders,obstacle detection,Obstacle detection,quadrocopter,quadrotor,quadrotors,Quadrotors,Sensors,UAV,ultrasonic,unmanned aerial vehicles,Unmanned aerial vehicles},
  pages = {599-609}
}

@inproceedings{Moreno-Armendariz.Calvo:ICMEAE:2014,
  title = {Visual {{SLAM}} and {{Obstacle Avoidance}} in {{Real Time}} for {{Mobile Robots Navigation}}},
  doi = {10.1109/ICMEAE.2014.12},
  abstract = {An important objective of an autonomous vehicle is to navigate through an unknown environment. A method used to achieve this objective is to generate a map. A map provides the means for the vehicle to create paths between the visited places autonomously in order to perform a task. A particular problem is to obtain such a map when there is no initial knowledge of the surroundings or not even the initial position of the robot in the environment. On other hand, avoiding static and dynamic obstacles is required, so a novel artificial potential field method is presented. The new designs that solve both problems are implemented on an FPGA. The novel designs are then tested on differential traction mobile robots with a computer vision system that travel on a controlled unknown environment. The experimental results show good performance in real time.},
  booktitle = {2014 {{International Conference}} on {{Mechatronics}}, {{Electronics}} and {{Automotive Engineering}}},
  author = {Moreno-Armend{\'a}riz, M. A. and Calvo, H.},
  month = nov,
  year = {2014},
  keywords = {artificial potential field method,autonomous vehicle,collision avoidance,Collision avoidance,Computer Vision,computer vision system,differential traction mobile robots,dynamic obstacles,field programmable gate arrays,Field programmable gate arrays,FPGA,FPGA implementation,Mathematical model,mobile robots,Mobile robots,mobile robots navigation,navigation,obstacle avoidance,robot vision,Simultaneous localization and mapping,SLAM (robots),SLAM algorithm,visual SLAM},
  pages = {44-49}
}

@article{Tomic.Schmid.ea:IRAM:2012,
  title = {Toward a {{Fully Autonomous UAV}}: {{Research Platform}} for {{Indoor}} and {{Outdoor Urban Search}} and {{Rescue}}},
  volume = {19},
  issn = {1070-9932},
  shorttitle = {Toward a {{Fully Autonomous UAV}}},
  doi = {10.1109/MRA.2012.2206473},
  abstract = {Urban search and rescue missions raise special requirements on robotic systems. Small aerial systems provide essential support to human task forces in situation assessment and surveillance. As external infrastructure for navigation and communication is usually not available, robotic systems must be able to operate autonomously. A limited payload of small aerial systems poses a great challenge to the system design. The optimal tradeoff between flight performance, sensors, and computing resources has to be found. Communication to external computers cannot be guaranteed; therefore, all processing and decision making has to be done on board. In this article, we present an unmanned aircraft system design fulfilling these requirements. The components of our system are structured into groups to encapsulate their functionality and interfaces. We use both laser and stereo vision odometry to enable seamless indoor and outdoor navigation. The odometry is fused with an inertial measurement unit in an extended Kalman filter. Navigation is supported by a module that recognizes known objects in the environment. A distributed computation approach is adopted to address the computational requirements of the used algorithms. The capabilities of the system are validated in flight experiments, using a quadrotor.},
  number = {3},
  journal = {IEEE Robotics Automation Magazine},
  author = {Tomic, T. and Schmid, K. and Lutz, P. and Domel, A. and Kassecker, M. and Mair, E. and Grixa, I. L. and Ruess, F. and Suppa, M. and Burschka, D.},
  month = sep,
  year = {2012},
  keywords = {aerial systems,aircraft control,Aircraft navigation,autonomous aerial vehicles,Cameras,computational requirements,computing resources,decision making,distributed computation approach,Emergency services,extended Kalman filter,flight performance,fully autonomous UAV,indoor navigation,Kalman filters,laser vision,Measurement by laser beam,outdoor navigation,Position measurement,Remotely operated vehicles,rescue missions,robotic system,Robots,sensors,stereo vision,system design,Unmanned aerial vehicles,unmanned aircraft system design,Urban areas,urban search},
  pages = {46-56}
}

@inproceedings{Rodriguez.Castiblanco.ea:ICUAS:2014,
  title = {Low-Cost Quadrotor Applied for Visual Detection of Landmine-like Objects},
  doi = {10.1109/ICUAS.2014.6842242},
  abstract = {This paper presents the use of a low-cost quadrotor applied for visual detection of landmine-like objects. Nowadays, landmines in Colombia are harmful and even letal artifacts mostly abandoned in rural areas. A percentage of the overall number of landmines are hand-crafted and partially exposed on the terrain's surface so that they can be triggered. Based on that fact, we propose an artificial vision approach as a complementary tool for landmine detection. To this purpose, we have developed an open-source software package based on the Robot Operating System (ROS) platform that can be easily: (i) integrated with a low-cost quadrotor such as the AR.drone parrot 2.0 and (ii) applied for real-time visual detection of landmine-like objects (tuna cans). This article describes the ROS-based package called drone detection and the experiments carried out for assessing the detection performance for different type of terrains and landmine visibility.},
  booktitle = {2014 {{International Conference}} on {{Unmanned Aircraft Systems}} ({{ICUAS}})},
  author = {Rodriguez, J. and Castiblanco, C. and Mondragon, I. and Colorado, J.},
  month = may,
  year = {2014},
  keywords = {aerospace computing,AR.drone parrot 2.0,artificial vision approach,Cameras,Colombia,computer vision,drone detection,Feature extraction,helicopters,landmine detection,Landmine detection,landmine-like object visual detection,low-cost quadrotor,military computing,Navigation,Noise,Open source software,open-source software package,operating systems (computers),public domain software,robot operating system platform,ROS-based package,Visualization},
  pages = {83-88}
}

@inproceedings{Gaszczak.Breckon.ea:IRCV:2011,
  title = {Real-Time People and Vehicle Detection from {{UAV}} Imagery},
  volume = {7878},
  doi = {10.1117/12.876663},
  abstract = {A generic and robust approach for the real-time detection of people and vehicles from an Unmanned Aerial Vehicle (UAV) is an important goal within the framework of fully autonomous UAV deployment for aerial reconnaissance and surveillance. Here we present an approach for the automatic detection of vehicles based on using multiple trained cascaded Haar classifiers with secondary confirmation in thermal imagery. Additionally we present a related approach for people detection in thermal imagery based on a similar cascaded classification technique combining additional multivariate Gaussian shape matching. The results presented show the successful detection of vehicle and people under varying conditions in both isolated rural and cluttered urban environments with minimal false positive detection. Performance of the detector is optimized to reduce the overall false positive rate by aiming at the detection of each object of interest (vehicle/person) at least once in the environment (i.e. per search patter flight path) rather than every object in each image frame. Currently the detection rate for people is \textasciitilde{}70\% and cars \textasciitilde{}80\% although the overall episodic object detection rate for each flight pattern exceeds 90\%.},
  booktitle = {Intelligent {{Robots}} and {{Computer Vision XXVIII}}: {{Algorithms}} and {{Techniques}}},
  publisher = {{International Society for Optics and Photonics}},
  author = {Gaszczak, Anna and Breckon, Toby P. and Han, Jiwan},
  month = jan,
  year = {2011},
  pages = {78780B}
}

@inproceedings{Saif.Prabuwono.ea:IROS:2013,
  series = {Communications in Computer and Information Science},
  title = {Real {{Time Vision Based Object Detection}} from {{UAV Aerial Images}}: {{A Conceptual Framework}}},
  isbn = {978-3-642-40408-5 978-3-642-40409-2},
  shorttitle = {Real {{Time Vision Based Object Detection}} from {{UAV Aerial Images}}},
  doi = {10.1007/978-3-642-40409-2_23},
  abstract = {In computer vision research, one of the capabilities of establishing an autonomous UAV is the detection of rigid and non-rigid object. Moving object detection with moving cameras from UAV aerial images is still an unsolved issue due to clutter and rural background contained in the images, even and uneven illumination changes, static and moving objects and motion of camera. This paper presents a conceptual framework for moving object detection with moving camera from UAV aerial images combined with the frame difference and segmentation approach together. Our focus is the human as rigid and vehicle as non rigid object detection where the camera can be mounted on the vehicle or other movable platform. It is expected that the proposed conceptual framework performs well under different situations for uneven environments.},
  language = {en},
  booktitle = {Intelligent {{Robotics Systems}}: {{Inspiring}} the {{NEXT}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Saif, A. F. M. Saifuddin and Prabuwono, Anton Satria and Mahayuddin, Zainal Rasyid},
  month = aug,
  year = {2013},
  pages = {265-274}
}

@inproceedings{Granlund.Nordberg.ea:ITCE:2000,
  title = {Witas: {{An}} Intelligent Autonomous Aircraft Using Active Vision},
  booktitle = {In {{Proceedings}} of the {{UAV}} 2000 {{International Technical Conference}} and {{Exhibition}}},
  author = {Granlund, G{\"o}sta and Nordberg, Klas and Wiklund, Johan and Doherty, Patrick and Skarman, Erik and S, Erik},
  year = {2000}
}

@inproceedings{Kaaniche.Champion.ea:ICRA:2005,
  title = {A {{Vision Algorithm}} for {{Dynamic Detection}} of {{Moving Vehicles}} with a {{UAV}}},
  doi = {10.1109/ROBOT.2005.1570387},
  abstract = {This paper presents a vision system for road traffic surveillance from sequences acquired from an unmanned aerial vehicle (UAV). This UAV is able to follow a path considered as the surveillance area and defined by a set of ordered GPS points. During the navigation of the UAV, the vision system acquires sequences which are treated in real-time in order to detect vehicles. This detection allows to perform a traffic estimation or to track a pointed out vehicle. The detection of vehicles is based on the spatiotemporal grouping of primitives formulated as a normalized cuts problem. A verification step based on the Dempster-Shafer theory is also proposed in order to recognize the vehicles.},
  booktitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kaaniche, K. and Champion, B. and Pegard, C. and Vasseur, P.},
  month = apr,
  year = {2005},
  keywords = {Corner Matching,Dempster-Shafer,Global Positioning System,Heuristic algorithms,Machine vision,Navigation,Normalized Cuts,Real time systems,Roads,Surveillance,UAV,Unmanned aerial vehicles,Vehicle detection,Vehicle Detection,Vehicle dynamics},
  pages = {1878-1883}
}

@article{Bradley.Roth:ACM:2007,
  title = {G.: {{Adaptive}} Thresholding Using the Integral Image},
  shorttitle = {G.},
  abstract = {Figure 1: Real-time adaptive image thresholding. Left: Input image. Center: Wellner's previous technique. Right:},
  journal = {ACM J. Graph. Tools},
  author = {Bradley, Derek and Roth, Gerhard},
  year = {2007},
  pages = {13--21}
}

@article{Li.Lee:ICPR:1993,
  title = {Minimum Cross Entropy Thresholding},
  volume = {26},
  issn = {0031-3203},
  doi = {10.1016/0031-3203(93)90115-D},
  abstract = {The threshold selection problem is solved by minimizing the cross entropy between the image and its segmented version. The cross entropy is formulated in a pixel-to-pixel basis between the two images and a computationally attractive algorithm employing the histogram is developed. Without making a priori assumptions about the population distribution, this method provides an unbiased estimate of a binarized version of the image in an information theoretic sense.},
  number = {4},
  journal = {Pattern Recognition},
  author = {Li, C. H. and Lee, C. K.},
  month = apr,
  year = {1993},
  keywords = {Image segmentation,Maximum entropy method,Minimum cross entropy,Thresholding},
  pages = {617-625}
}

@article{Yen.Chang.ea:TIP:1995,
  title = {A New Criterion for Automatic Multilevel Thresholding},
  volume = {4},
  issn = {1057-7149},
  doi = {10.1109/83.366472},
  abstract = {A new criterion for multilevel thresholding is proposed. The criterion is based on the consideration of two factors. The first one is the discrepancy between the thresholded and original images and the second one is the number of bits required to represent the thresholded image. Based on a new maximum correlation criterion for bilevel thresholding, the discrepancy is defined and then a cost function that takes both factors into account is proposed for multilevel thresholding. By minimizing the cost function, the classification number that the gray-levels should be classified and the threshold values can be determined automatically. In addition, the cost function is proven to possess a unique minimum under very mild conditions. Computational analyses indicate that the number of required mathematical operations in the implementation of our algorithm is much less than that of maximum entropy criterion. Finally, simulation results are included to demonstrate their effectiveness},
  number = {3},
  journal = {IEEE Transactions on Image Processing},
  author = {Yen, Jui-Cheng and Chang, Fu-Juay and Chang, Shyang},
  month = mar,
  year = {1995},
  keywords = {Algorithm design and analysis,Analytical models,Application software,automatic multilevel thresholding,bilevel thresholding,Computational complexity,Computational modeling,Computer industry,correlation methods,cost function,Cost function,Entropy,image classification,image processing,Image processing,mathematical operations,maximum correlation criterion,simulation results,Target recognition,threshold values,thresholded image},
  pages = {370-378}
}

@book{Niblack:ImageProcc:1986,
  title = {An {{Introduction}} to {{Digital Image Processing}}},
  isbn = {978-0-13-480674-7},
  language = {en},
  publisher = {{Prentice-Hall}},
  author = {Niblack, Wayne},
  year = {1986},
  keywords = {Computers / Image Processing,Technology \& Engineering / General,Technology \& Engineering / Imaging Systems}
}

@article{Sauvola.Pietikainen:ICPR:2000,
  title = {Adaptive Document Image Binarization},
  volume = {33},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(99)00055-2},
  abstract = {A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.},
  number = {2},
  journal = {Pattern Recognition},
  author = {Sauvola, J. and Pietik{\"a}inen, M.},
  month = feb,
  year = {2000},
  keywords = {Adaptive binarization,Document analysis,Document segmentation,Document understanding,Soft decision},
  pages = {225-236}
}

@article{Wu.Arkhipov.ea:TC:2017,
  title = {{{ADDSEN}}: {{Adaptive Data Processing}} and {{Dissemination}} for {{Drone Swarms}} in {{Urban Sensing}}},
  volume = {66},
  issn = {0018-9340},
  shorttitle = {{{ADDSEN}}},
  doi = {10.1109/TC.2016.2584061},
  abstract = {We present ADDSEN middleware as a holistic solution for Adaptive Data processing and dissemination for Drone swarms in urban SENsing. To efficiently process sensed data in the middleware, we have proposed a cyber-physical sensing framework using partially ordered knowledge sharing for distributed knowledge management in drone swarms. A reinforcement learning dissemination strategy is implemented in the framework. ADDSEN uses online learning techniques to adaptively balance the broadcast rate and knowledge loss rate periodically. The learned broadcast rate is adapted by executing state transitions during the process of online learning. A strategy function guides state transitions, incorporating a set of variables to reflect changes in link status. In addition, we design a cooperative dissemination method for the task of balancing storage and energy allocation in drone swarms. We implemented ADDSEN in our cyber-physical sensing framework, and evaluation results show that it can achieve both maximal adaptive data processing and dissemination performance, presenting better results than other commonly used dissemination protocols such as periodic, uniform and neighbor protocols in both single-swarm and multi-swarm cases.},
  number = {2},
  journal = {IEEE Transactions on Computers},
  author = {Wu, D. and Arkhipov, D. I. and Kim, M. and Talcott, C. L. and Regan, A. C. and McCann, J. A. and Venkatasubramanian, N.},
  month = feb,
  year = {2017},
  keywords = {adaptive data processing,ADDSEN middleware,autonomous aerial vehicles,control engineering computing,cooperative dissemination,cyber-physical sensing framework,cyber-physical systems,data dissemination,data handling,data processing,Data processing,distributed knowledge management,drone swarm,Drone swarms,energy allocation balancing,knowledge management,knowledge sharing,learning (artificial intelligence),Learning (artificial intelligence),middleware,Middleware,Mobile communication,mobile robots,multi-robot systems,online learning technique,online Q-learning,reinforcement learning dissemination strategy,resource allocation,Sensors,storage allocation balancing,swarm intelligence,UAV,unmanned aerial vehicle,urban sensing,Vehicles},
  pages = {183-198}
}

@inproceedings{Huang:PMIS:2007,
  address = {Washington, D.C.},
  series = {PerMIS '07},
  title = {Autonomy {{Levels}} for {{Unmanned Systems}} ({{ALFUS}}) {{Framework}}: {{Safety}} and {{Application Issues}}},
  isbn = {978-1-59593-854-1},
  doi = {10.1145/1660877.1660883},
  booktitle = {Proceedings of the 2007 {{Workshop}} on {{Performance Metrics}} for {{Intelligent Systems}}},
  publisher = {{ACM}},
  author = {Huang, Hui-Min},
  year = {2007},
  keywords = {autonomy,contextual autonomous capability (CAC),environment,human independence (HI),human robot interaction (HRI),metrics,mission,task,unmanned system (UMS)},
  pages = {48-53}
}

@article{Ridler.Calvard:TSMC:1978,
  title = {Picture {{Thresholding Using}} an {{Iterative Selection Method}}},
  volume = {8},
  number = {8},
  journal = {Systems, Man and Cybernetics, IEEE Transactions on},
  author = {Ridler, T.W. and Calvard, S.},
  year = {1978},
  pages = {630-632}
}

@inproceedings{Hrabar.Sukhatme:IROS:2004,
  title = {A Comparison of Two Camera Configurations for Optic-Flow Based Navigation of a {{UAV}} through Urban Canyons},
  volume = {3},
  doi = {10.1109/IROS.2004.1389812},
  abstract = {We present a comparison of two camera configurations for avoiding obstacles in 3D-space using optic flow. The two configurations were developed for use on an autonomous helicopter, with the aim of enabling it to fly in environments with tall obstacles (e.g. urban canyons). The comparison is made based on real data captured from two sideways-looking cameras and an omnidirectional camera mounted onboard an autonomous helicopter. Optic flow information from the images is used to determine the relative distance to obstacles on each side of the helicopter. We show that on average, both camera configurations are equally effective and that they can be used to tell which of the canyon walls is closer with an accuracy of 74\%. It is noted that each configuration is however more effective under certain conditions, and so a suitable hybrid approach is suggested. We also show that there is a linear relationship between the optic flow ratios and the position of the helicopter with respect to the center of the canyon. We use this relationship to develop a proportional control strategy for flying the helicopter along the Voronoi line between buildings.},
  booktitle = {International {{Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Hrabar, S. and Sukhatme, G. S.},
  month = sep,
  year = {2004},
  keywords = {Testing,Laboratories,Cameras,aerospace robotics,Aircraft navigation,autonomous helicopter,Avatars,helicopters,Helicopters,mobile robots,remotely operated vehicles,Unmanned aerial vehicles,UAV,cameras,Robot vision systems,collision avoidance,navigation,camera configurations,Embedded system,Image motion analysis,omnidirectional camera,optic-flow based navigation,proportional control strategy,sideways-looking cameras,urban canyons,Voronoi line},
  pages = {2673-2680 vol.3}
}

@inproceedings{Johnson.Montgomery.ea:ICRA:2005,
  title = {Vision {{Guided Landing}} of an {{Autonomous Helicopter}} in {{Hazardous Terrain}}},
  doi = {10.1109/ROBOT.2005.1570727},
  abstract = {Future robotic space missions will employ a precision soft-landing capability that will enable exploration of previously inaccessible sites that have strong scientific significance. To enable this capability, a fully autonomous onboard system that identifies and avoids hazardous features such as steep slopes and large rocks is required. Such a system will also provide greater functionality in unstructured terrain to unmanned aerial vehicles. This paper describes an algorithm for landing hazard avoidance based on images from a single moving camera. The core of the algorithm is an efficient application of structure from motion to generate a dense elevation map of the landing area. Hazards are then detected in this map and a safe landing site is selected. The algorithm has been implemented on an autonomous helicopter testbed and demonstrated four times resulting in the first autonomous landing of an unmanned helicopter in unknown and hazardous terrain.},
  booktitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Johnson, A. and Montgomery, J. and Matthies, L.},
  month = apr,
  year = {2005},
  keywords = {Testing,Laboratories,Cameras,Helicopters,Unmanned aerial vehicles,UAV,autonomous landing,hazard detection,Hazards,Propulsion,Space missions,Space technology,Space vehicles,structure from motion},
  pages = {3966-3971}
}

@incollection{Huang.Bachrach.ea:RobR:2017,
  series = {Springer Tracts in Advanced Robotics},
  title = {Visual {{Odometry}} and {{Mapping}} for {{Autonomous Flight Using}} an {{RGB}}-{{D Camera}}},
  isbn = {978-3-319-29362-2 978-3-319-29363-9},
  abstract = {RGB-D cameras provide both a color image and per-pixel depth estimates. The richness of their data and the recent development of low-cost sensors have combined to present an attractive opportunity for mobile robotics research. In this paper, we describe a system for visual odometry and mapping using an RGB-D camera, and its application to autonomous flight. By leveraging results from recent state-of-the-art algorithms and hardware, our system enables 3D flight in cluttered environments using only onboard sensor data. All computation and sensing required for local position control are performed onboard the vehicle, reducing the dependence on unreliable wireless links. We evaluate the effectiveness of our system for stabilizing and controlling a quadrotor micro air vehicle, demonstrate its use for constructing detailed 3D maps of an indoor environment, and discuss its limitations.},
  language = {en},
  booktitle = {Robotics {{Research}}},
  publisher = {{Springer, Cham}},
  author = {Huang, Albert S. and Bachrach, Abraham and Henry, Peter and Krainin, Michael and Maturana, Daniel and Fox, Dieter and Roy, Nicholas},
  year = {2017},
  pages = {235-252}
}

@article{Padhy.Xia.ea:TSC:2018,
  title = {Monocular {{Vision Aided Autonomous UAV Navigation}} in {{Indoor Corridor Environments}}},
  doi = {10.1109/TSUSC.2018.2810952},
  abstract = {Deployment of autonomous Unmanned Aerial Vehicles (UAV) in various sectors such as disaster hit environments, industries, agriculture etc. not only improves productivity but also reduces human intervention resulting in sustainable benefits. In this regard, we present a model for autonomous navigation and collision avoidance of UAVs in GPS-denied corridor environments. In the first stage, we suggest a fast procedure to estimate the set of parallel lines whose intersection would yield the position of the vanishing point (VP) inside the corridor. A suitable measure is then formulated based on the position of VP on the intersecting lines in reference to any of the image boundary axes which helps safe navigation of the UAV avoiding collisions with side walls. Furthermore, the relative Euclidean distance scale expansion of matched scale-invariant keypoints in a pair of frames is taken into account to estimate the depth of a frontal obstacle. However, turbulence in the UAV arising due to its rotors or external factors may introduce uncertainty in depth estimation. It is rectified with the help of a constant velocity aided Kalman filter model. Necessary set of control commands are then generated to avoid the frontal collision. Exhaustive experiments advocate the efficacy of the proposed scheme.},
  journal = {IEEE Transactions on Sustainable Computing},
  author = {Padhy, R. P. and Xia, F. and Choudhury, S. K. and Sa, P. K. and Bakshi, S.},
  year = {2018},
  keywords = {Cameras,Unmanned aerial vehicles,Kalman filter,Navigation,Collision avoidance,Robot sensing systems,Autonomous robots,Monocular vision,Scale-invariant features,Task analysis,UAV navigation,Vanishing point},
  pages = {1-1}
}

@inproceedings{Seitz.Curless.ea:CVPR:2006,
  title = {A {{Comparison}} and {{Evaluation}} of {{Multi}}-{{View Stereo Reconstruction Algorithms}}},
  volume = {1},
  doi = {10.1109/CVPR.2006.19},
  abstract = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.},
  booktitle = {Computer {{Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'06)},
  author = {Seitz, S. M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.},
  month = jun,
  year = {2006},
  keywords = {Layout,Shape measurement,Image databases,Cameras,Image reconstruction,Educational institutions,Reconstruction algorithms,Stereo image processing,Stereo vision,Taxonomy},
  pages = {519-528}
}

@inproceedings{Thrun.Bu:AI:1996,
  address = {Portland, Oregon},
  series = {AAAI'96},
  title = {Integrating {{Grid}}-Based and {{Topological Maps}} for {{Mobile Robot Navigation}}},
  isbn = {978-0-262-51091-2},
  abstract = {Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are considerably difficult to learn in large-scale environments. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms--grid-based and topological--, the approach presented here gains the best of both worlds: accuracy/consistency and efficiency. The paper gives results for autonomously operating a mobile robot equipped with sonar sensors in populated multi-room environments.},
  booktitle = {Proceedings of the {{Thirteenth National Conference}} on {{Artificial Intelligence}} - {{Volume}} 2},
  publisher = {{AAAI Press}},
  author = {Thrun, Sebastian and B{\"u}, Arno},
  year = {1996},
  pages = {944--950}
}

@inproceedings{Li.Ye.ea:IROS:2016,
  title = {Multi-Target Detection and Tracking from a Single Camera in {{Unmanned Aerial Vehicles}} ({{UAVs}})},
  doi = {10.1109/IROS.2016.7759733},
  abstract = {Despite the recent flight control regulations, Unmanned Aerial Vehicles (UAVs) are still gaining popularity in civilian and military applications, as much as for personal use. Such emerging interest is pushing the development of effective collision avoidance systems. Such systems play a critical role UAVs operations especially in a crowded airspace setting. Because of cost and weight limitations associated with UAVs payload, camera based technologies are the de-facto choice for collision avoidance navigation systems. This requires multi-target detection and tracking algorithms from a video, which can be run on board efficiently. While there has been a great deal of research on object detection and tracking from a stationary camera, few have attempted to detect and track small UAVs from a moving camera. In this paper, we present a new approach to detect and track UAVs from a single camera mounted on a different UAV. Initially, we estimate background motions via a perspective transformation model and then identify distinctive points in the background subtracted image. We find spatio-temporal traits of each moving object through optical flow matching and then classify those candidate targets based on their motion patterns compared with the background. The performance is boosted through Kalman filter tracking. This results in temporal consistency among the candidate detections. The algorithm was validated on video datasets taken from a UAV. Results show that our algorithm can effectively detect and track small UAVs with limited computing resources.},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Li, J. and Ye, D. H. and Chung, T. and Kolsch, M. and Wachs, J. and Bouman, C.},
  month = oct,
  year = {2016},
  keywords = {Target tracking,image classification,image matching,Cameras,image sequences,object detection,mobile robots,robot vision,Unmanned aerial vehicles,autonomous aerial vehicles,Object detection,unmanned aerial vehicles,cameras,image filtering,Kalman filters,single camera,image motion analysis,object tracking,collision avoidance,Collision avoidance,aerospace navigation,background motion estimation,background subtracted image,camera based technology,civilian application,collision avoidance navigation system,collision avoidance system,crowded airspace setting,flight control regulations,Kalman filter tracking,military application,motion pattern,moving camera,multitarget detection,multitarget tracking,optical flow matching,Optical imaging,perspective transformation model,small UAV tracking,spatio-temporal trait,stationary camera,target classification,temporal consistency,UAV operation,UAV payload,video dataset,video signal processing},
  pages = {4992-4997}
}

@article{Huh.Shim:IROS:2010,
  title = {A {{Vision}}-{{Based Automatic Landing Method}} for {{Fixed}}-{{Wing UAVs}}},
  volume = {57},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-009-9382-2},
  abstract = {In this paper, a vision-based landing system for small-size fixed-wing unmanned aerial vehicles (UAVs) is presented. Since a single GPS without a differential correction typically provide position accuracy of at most a few meters, an airplane equipped with a single GPS only is not guaranteed to land at a designated location with a sufficient accuracy. Therefore, a visual servoing algorithm is proposed to improve the accuracy of landing. In this scheme, the airplane is controlled to fly into the visual marker by directly feeding back the pitch and yaw deviation angles sensed by the forward-looking camera during the terminal landing phase. The visual marker is a monotone hemispherical airbag, which serves as the arresting device while providing a strong and yet passive visual cue for the vision system. The airbag is detected by using color- and moment-based target detection methods. The proposed idea was tested in a series of experiments using a blended wing-body airplane and proven to be viable for landing of small fixed-wing UAVs.},
  language = {en},
  number = {1-4},
  journal = {Journal of Intelligent and Robotic Systems},
  author = {Huh, Sungsik and Shim, David Hyunchul},
  month = jan,
  year = {2010},
  pages = {217}
}

@article{Marianandam.Ghose:IFAC:2014,
  series = {3rd International Conference on Advances in Control and Optimization of Dynamical Systems (2014)},
  title = {Vision {{Based Alignment}} to {{Runway}} during {{Approach}} for {{Landing}} of {{Fixed}} Wing {{UAVs}}},
  volume = {47},
  issn = {1474-6670},
  doi = {10.3182/20140313-3-IN-3024.00197},
  abstract = {Aligning a fixed wing UAV to the runway using vision as the only sensor has been implemented. FlightGear and Matlab is interfaced to develop a Vision-in-the-Loop Simulation facility. Interfacing the image to control in real time has been successfully implemented to autonomously align the UAV to the runway. Image processing is performed to detect and track the runway lines as visual cues using Hough Transforms. An image based proportional controller consisting of course and fine roll control inputs, akin to a manned aircraft pilot input has been developed for UAV alignment. Simulation results with varying heading off-set to the runway have been presented.},
  number = {1},
  journal = {IFAC Proceedings Volumes},
  author = {Marianandam, Peter Arun and Ghose, D},
  month = jan,
  year = {2014},
  keywords = {Canny Edge Detection,Conventional Landing,Fixed Wing UAV,Hough Transform,Image Processing,Runway Alignment,Vision-in-the-Loop},
  pages = {470-476}
}

@inproceedings{Miller.Shah.ea:ICRA:2008,
  title = {Landing a {{UAV}} on a Runway Using Image Registration},
  doi = {10.1109/ROBOT.2008.4543206},
  abstract = {In this paper we present a system that uses only vision to land a UAV on a runway. We describe a method for estimating the relative location of the runway as an image by performing image registration against a stack of images in which the location of the runway is known. An approximation of the camera projection model for a forward-facing view of a runway is derived, allowing the course deviation of the UAV to be estimated from a registered image. The course deviation is used as input to a linear feedback control loop to maintain the correct flight path. Our method is implemented as a real-time multithreaded application, which is used to control an aircraft in Microsoft Flight Simulator. We also show results of applying the vision component of the system to video recorded from an actual UAV.},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Miller, A. and Shah, M. and Harper, D.},
  month = may,
  year = {2008},
  keywords = {Cameras,Aircraft navigation,Global Positioning System,remotely operated vehicles,Unmanned aerial vehicles,Aerospace control,aerospace simulation,aircraft,camera projection model,course deviation,feedback,Feedback control,flight path,Geometry,image registration,Image registration,linear feedback control loop,Microsoft Flight Simulator,real-time multithreaded application,Robotics and automation,UAV landing,USA Councils},
  pages = {182-187}
}

@inproceedings{Laiacker.Kondak.ea:IROS:2013,
  title = {Vision Aided Automatic Landing System for Fixed Wing {{UAV}}},
  doi = {10.1109/IROS.2013.6696777},
  abstract = {In this paper, we present a multi-sensor system for automatic landing of fixed wing UAVs. The system is composed of a high precision aircraft controller and a vision module which is currently used for detection and tracking of runways. Designing the system we paid special attention to its robustness. The runway detection algorithm uses a maximum amount of information in images and works with high level geometrical models. It allows detecting a runway under different weather conditions even if only a small part is visible in the image. In order to increase landing reliability under sub-optimal wind conditions, an additional loop was introduced into the altitude controller. All control and image processing is performed onboard. The system has been successfully tested in flight experiments with two different fixed wing platforms at various weather conditions, in summer, fall and winter.},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Laiacker, M. and Kondak, K. and Schwarzbach, M. and Muskardin, T.},
  month = nov,
  year = {2013},
  keywords = {Detection algorithms,image processing,Cameras,object detection,Global Positioning System,robot vision,autonomous aerial vehicles,object tracking,Aircraft,aerospace components,aircraft landing guidance,altitude controller,Atmospheric modeling,Elevators,environmental factors,fixed wing UAV,high level geometrical models,high precision aircraft controller,image fusion,landing reliability,multisensor system,reliability,runway detection algorithm,runway tracking,suboptimal wind conditions,Trajectory,vision aided automatic landing system,vision module,weather conditions},
  pages = {2971-2976}
}

@article{Eberli.Scaramuzza.ea:IROS:2011,
  title = {Vision {{Based Position Control}} for {{MAVs Using One Single Circular Landmark}}},
  volume = {61},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-010-9494-8},
  abstract = {This paper presents a real-time vision based algorithm for 5 degrees-of-freedom pose estimation and set-point control for a Micro Aerial Vehicle (MAV). The camera is mounted on-board a quadrotor helicopter. Camera pose estimation is based on the appearance of two concentric circles which are used as landmark. We show that that by using a calibrated camera, conic sections, and the assumption that yaw is controlled independently, it is possible to determine the six degrees-of-freedom pose of the MAV. First we show how to detect the landmark in the image frame. Then we present a geometric approach for camera pose estimation from the elliptic appearance of a circle in perspective projection. Using this information we are able to determine the pose of the vehicle. Finally, given a set point in the image frame we are able to control the quadrotor such that the feature appears in the respective target position. The performance of the proposed method is presented through experimental results.},
  language = {en},
  number = {1-4},
  journal = {Journal of Intelligent \& Robotic Systems},
  author = {Eberli, Daniel and Scaramuzza, Davide and Weiss, Stephan and Siegwart, Roland},
  month = jan,
  year = {2011},
  pages = {495-512}
}

@misc{ASTechWeb,
  title = {{ASTech Paris Region - Accueil}},
  language = {fr},
  howpublished = {https://www.pole-astech.org}
}

@misc{ROSWeb,
  title = {{{ROS}}.Org | {{Powering}} the World's Robots},
  howpublished = {http://www.ros.org/}
}

@misc{GazeboWeb,
  title = {Gazebo},
  howpublished = {http://gazebosim.org/}
}

@article{Barbot.Landy.ea:JV:2012,
  title = {Differential Effects of Exogenous and Endogenous Attention on Second-Order Texture Contrast Sensitivity},
  volume = {12},
  issn = {1534-7362},
  doi = {10.1167/12.8.6},
  language = {en},
  number = {8},
  journal = {Journal of Vision},
  author = {Barbot, Antoine and Landy, Michael S. and Carrasco, Marisa},
  month = aug,
  year = {2012},
  pages = {6-6}
}

@inproceedings{Jeulin:ISMM:2015,
  series = {Lecture Notes in Computer Science},
  title = {Probabilistic {{Hierarchical Morphological Segmentation}} of {{Textures}}},
  isbn = {978-3-319-18719-8 978-3-319-18720-4},
  doi = {10.1007/978-3-319-18720-4_27},
  abstract = {A general methodology is introduced for texture segmentation in binary, scalar, or multispectral images. Textural information is obtained from morphological operations of images. Starting from a fine partition of the image in regions, hierarchical segmentations are designed in a probabilistic framework by means of probabilistic distances conveying the textural information, and of random markers accounting for the morphological content of the regions and of their spatial arrangement.},
  language = {en},
  booktitle = {Mathematical {{Morphology}} and {{Its Applications}} to {{Signal}} and {{Image Processing}}},
  publisher = {{Springer, Cham}},
  author = {Jeulin, Dominique},
  month = may,
  year = {2015},
  pages = {313-324}
}

@article{Beucher.Meyer:OI:1993,
  title = {The Morphological Approach to Segmentation: The Watershed Transformation. {{Mathematical}} Morphology in Image Processing.},
  volume = {34},
  journal = {Optical Engineering},
  author = {Beucher, S. and Meyer, F.},
  year = {1993},
  pages = {433-481}
}

@misc{ParrotWeb,
  title = {{Site Officiel Parrot\textregistered{}}},
  abstract = {Drones avec cam{\'e}ra \& applications},
  language = {fr},
  howpublished = {https://www.parrot.com/fr/},
  journal = {Site Officiel Parrot\textregistered{}}
}

@incollection{Mosler:Springer:2013,
  title = {Depth {{Statistics}}},
  isbn = {978-3-642-35493-9 978-3-642-35494-6},
  abstract = {In 1975 John Tukey proposed a multivariate median which is the `deepest' point in a given data cloud in $\mathbb{R}$ d . Later, in measuring the depth of an arbitrary point z with respect to the data, David Donoho and Miriam Gasko considered hyperplanes through z and determined its `depth' by the smallest portion of data that are separated by such a hyperplane. Since then, these ideas have proved extremely fruitful. A rich statistical methodology has developed that is based on data depth and, more general, nonparametric depth statistics. General notions of data depth have been introduced as well as many special ones. These notions vary regarding their computability and robustness and their sensitivity to reflect asymmetric shapes of the data. According to their different properties they fit to particular applications. The upper level sets of a depth statistic provide a family of set-valued statistics, named depth-trimmed or central regions. They describe the distribution regarding its location, scale and shape. The most central region serves as a median. The notion of depth has been extended from data clouds, that is empirical distributions, to general probability distributions on $\mathbb{R}$ d , thus allowing for laws of large numbers and consistency results. It has also been extended from d-variate data to data in functional spaces.},
  language = {en},
  booktitle = {Robustness and {{Complex Data Structures}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Mosler, Karl},
  year = {2013},
  pages = {17-34}
}

@inproceedings{Dokladal:ISMM:2017,
  series = {Lecture Notes in Computer Science},
  title = {Statistical {{Threshold Selection}} for {{Path Openings}} to {{Detect Cracks}}},
  isbn = {978-3-319-57239-0 978-3-319-57240-6},
  doi = {10.1007/978-3-319-57240-6_30},
  abstract = {Inspired by the a contrario approach this paper proposes a way of setting the threshold when using parsimonious path filters to detect thin curvilinear structures in images.The a contrario approach, instead of modeling the structures to detect, models the noise to detect structures deviating from the model. In this scope, we assume noise composed of pixels that are independent random variables. Henceforth, cracks that are curvilinear sequences of bright pixels (not necessarily connected) are detected as abnormal sequences of bright pixels.In the second part, a fast approximation of the solution based on parsimonious path openings is shown.},
  language = {en},
  booktitle = {Mathematical {{Morphology}} and {{Its Applications}} to {{Signal}} and {{Image Processing}}},
  publisher = {{Springer, Cham}},
  author = {Dokl{\'a}dal, Petr},
  month = may,
  year = {2017},
  pages = {369-380}
}

@article{Desolneux.Moisan.ea:IJCV:2000,
  title = {Meaningful {{Alignments}}},
  volume = {40},
  issn = {0920-5691, 1573-1405},
  doi = {10.1023/A:1026593302236},
  abstract = {We propose a method for detecting geometric structures in an image, without any a priori information. Roughly speaking, we say that an observed geometric event is ``meaningful'' if the expectation of its occurences would be very small in a random image. We discuss the apories of this definition, solve several of them by introducing ``maximal meaningful events'' and analyzing their structure. This methodology is applied to the detection of alignments in images.},
  language = {en},
  number = {1},
  journal = {International Journal of Computer Vision},
  author = {Desolneux, Agn{\`e}s and Moisan, Lionel and Morel, Jean-Michel},
  month = oct,
  year = {2000},
  pages = {7-23}
}

@misc{InternestWeb,
  title = {Internest},
  howpublished = {http://internest.fr/fr/}
}

@techreport{BaquedanoA.:ESIEE:2017,
  type = {Project {{Report}}},
  title = {Automatic {{Drone Navigation Based}} on {{Computer Vision Applied}} to {{Drone Landing}}},
  abstract = {A report submitted in fulfillment of the requirements
for the degree of Telecommunications Engineering in Universidad P{\'u}blica de Navarra},
  institution = {{ESIEE Paris}},
  author = {{Baquedano, A.}},
  month = jun,
  year = {2017}
}


